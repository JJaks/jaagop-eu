name: Lighthouse Performance Testing

on:
  push:
    branches: [master]
  pull_request:
    branches: [master]

  workflow_dispatch:
    inputs:
      url:
        description: 'URL to test (defaults to production site)'
        required: false
        type: string

jobs:
  lighthouse:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: latest

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli@0.12.x

      # For manual runs with custom URL
      - name: Run Lighthouse CI (Custom URL)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.url
        run: |
          echo "Testing URL: ${{ github.event.inputs.url }}"

          # For manual runs, also test additional pages if it's a base URL
          BASE_URL="${{ github.event.inputs.url }}"
          if [[ "$BASE_URL" =~ ^https?://[^/]+/?$ ]]; then
            echo "Detected base URL, testing multiple pages..."
            PAGES=("" "/projects" "/blog")
            rm -rf .lighthouseci  # Clean slate
            
            for i in "${!PAGES[@]}"; do
              page="${PAGES[$i]}"
              URL="${BASE_URL%/}$page"
              echo "Testing: $URL"
              
              # Create temporary directory for this URL
              TEMP_DIR=".lighthouseci-temp-$i"
              mkdir -p "$TEMP_DIR"
              
              # Collect with temporary directory
              lhci collect \
                --url="$URL" \
                --numberOfRuns=3 \
                --startServerCommand="" \
                --staticDistDir="" \
                --settings.ci.collect.settings.chromeFlags="--no-sandbox --disable-dev-shm-usage" \
                --settings.ci.collect.outputDir="$TEMP_DIR"
            done
            
            # Combine all results into main .lighthouseci directory
            mkdir -p .lighthouseci
            for i in "${!PAGES[@]}"; do
              TEMP_DIR=".lighthouseci-temp-$i"
              if [ -d "$TEMP_DIR" ]; then
                cp "$TEMP_DIR"/*.json .lighthouseci/ 2>/dev/null || true
                rm -rf "$TEMP_DIR"
              fi
            done
          else
            echo "Testing single URL..."
            lhci collect --url="$BASE_URL" --numberOfRuns=3 --startServerCommand="" --staticDistDir=""
          fi

          # Run assertions but capture result
          set +e  # Don't exit on error
          # For manual testing, assume it's a preview URL if it contains typical preview patterns
          if [[ "$BASE_URL" =~ (vercel\.app|netlify\.app|preview|staging) ]]; then
            echo "Running assertions for preview deployment (ignoring is-crawlable)..."
            lhci assert --preset="lighthouse:no-pwa" --assertions.is-crawlable=off
          else
            echo "Running assertions for production..."
            lhci assert
          fi
          ASSERT_EXIT_CODE=$?
          set -e  # Re-enable exit on error

          # Always upload results
          lhci upload --target=temporary-public-storage

          # Handle assertion failure
          if [ $ASSERT_EXIT_CODE -ne 0 ]; then
            echo "Performance budgets not met, but continuing workflow"
            echo "Check the detailed report for performance issues"
          else
            echo "All performance budgets passed!"
          fi
        env:
          LHCI_BUILD_CONTEXT__CURRENT_HASH: ${{ github.sha }}
          LHCI_BUILD_CONTEXT__COMMIT_TIME: ${{ github.event.head_commit.timestamp }}
          LHCI_BUILD_CONTEXT__CURRENT_BRANCH: ${{ github.ref_name }}
          LHCI_BUILD_CONTEXT__COMMIT_MESSAGE: ${{ github.event.head_commit.message }}
          LHCI_SERVER_URL: ${{ github.event.inputs.url }}
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}

      - name: Get the Vercel preview url
        id: vercel_preview_url
        uses: zentered/vercel-preview-url@v1.4.0
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        with:
          vercel_project_id: ${{ secrets.VERCEL_PROJECT_ID }}
      # For automatic runs, we'll wait for deployment and test the live site
      - name: Wait for deployment (Vercel/Netlify)
        if: github.event_name != 'workflow_dispatch'
        uses: UnlyEd/github-action-await-vercel@v2.0.0
        id: await-vercel
        with:
          deployment-url: ${{ format('https://{0}', steps.vercel_preview_url.outputs.preview_url) }}
          timeout: 300
          poll-interval: 5
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        continue-on-error: true

      - name: Run Lighthouse CI
        if: github.event_name != 'workflow_dispatch'
        run: |
          set -euo pipefail
          # Try to determine the deployment URL
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            # For PRs, construct preview URL (adjust domain as needed)
            DEPLOY_URL=${{ format('https://{0}', steps.vercel_preview_url.outputs.preview_url) }}
          else
            # For main branch, use production URL (adjust as needed)
            DEPLOY_URL="https://jaagop.eu"
          fi
          echo "Testing base URL: $DEPLOY_URL"

          # Define pages to test
          PAGES=("" "/projects" "/blog")

          # Test each page with separate collect commands but ensure they don't overwrite
          echo "Testing multiple URLs..."
          rm -rf .lighthouseci  # Clean slate
          for i in "${!PAGES[@]}"; do
            page="${PAGES[$i]}"
            URL="$DEPLOY_URL$page"
            echo "Testing: $URL"
            
            # Create temporary directory for this URL
            TEMP_DIR=".lighthouseci-temp-$i"
            mkdir -p "$TEMP_DIR"
            
            # Collect with temporary directory
            LHCI_BUILD_CONTEXT__CURRENT_HASH="${{ github.sha }}" \
            LHCI_BUILD_CONTEXT__COMMIT_TIME="${{ github.event.head_commit.timestamp }}" \
            lhci collect \
              --url="$URL" \
              --numberOfRuns=3 \
              --startServerCommand="" \
              --staticDistDir="" \
              --settings.ci.collect.settings.chromeFlags="--no-sandbox --disable-dev-shm-usage" \
              --settings.ci.collect.outputDir="$TEMP_DIR"
          done

          # Combine all results into main .lighthouseci directory
          mkdir -p .lighthouseci
          for i in "${!PAGES[@]}"; do
            TEMP_DIR=".lighthouseci-temp-$i"
            if [ -d "$TEMP_DIR" ]; then
              cp "$TEMP_DIR"/*.json .lighthouseci/ 2>/dev/null || true
              rm -rf "$TEMP_DIR"
            fi
          done

          # Run assertions but capture result
          set +e  # Don't exit on error
          # For PR previews, ignore is-crawlable since they shouldn't be crawlable
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "Running assertions for PR preview (ignoring is-crawlable)..."
            lhci assert --preset="lighthouse:no-pwa" --assertions.is-crawlable=off --assertions.csp-xss=off --assertions.uses-passive-event-listeners=off
          else
            echo "Running assertions for production..."
            lhci assert
          fi
          ASSERT_EXIT_CODE=$?
          set -e  # Re-enable exit on error

          # Always upload results
          lhci upload --target=temporary-public-storage

          # Handle assertion failure
          if [ $ASSERT_EXIT_CODE -ne 0 ]; then
            echo "Performance budgets not met, but continuing workflow"
            echo "Check the detailed report for performance issues"
          else
            echo "All performance budgets passed!"
          fi

        env:
          LHCI_BUILD_CONTEXT__CURRENT_HASH: ${{ github.sha }}
          LHCI_BUILD_CONTEXT__COMMIT_TIME: ${{ github.event.head_commit.timestamp }}
          LHCI_BUILD_CONTEXT__CURRENT_BRANCH: ${{ github.ref_name }}
          LHCI_BUILD_CONTEXT__COMMIT_MESSAGE: ${{ github.event.head_commit.message }}
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}

      - name: Upload Lighthouse reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-reports-${{ github.sha }}
          path: '.lighthouseci/*'
          retention-days: 30
          include-hidden-files: true

      - name: Comment PR with Lighthouse results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            // Try to find the Lighthouse summary
            const lhciDir = '.lighthouseci';
            let summaryContent = '';
            try {
              if (fs.existsSync(lhciDir)) {
                const files = fs.readdirSync(lhciDir);
                
                // Look for individual lighthouse report files instead of manifest
                const reportFiles = files.filter(f => f.startsWith('lhr-') && f.endsWith('.json'));
                console.log('Found report files:', reportFiles);
                console.log('All files in .lighthouseci:', files);
                
                if (reportFiles.length > 0) {
                  summaryContent = `## ðŸš€ Lighthouse Performance Report

            **Tested ${reportFiles.length} page(s) for commit ${context.sha.substring(0, 8)}**

            `;
                  
                  // Check if there are assertion failures
                  const hasAssertionFailures = fs.existsSync('.lighthouseci/assertion-results.json');
                  if (hasAssertionFailures) {
                    try {
                      const assertionResults = JSON.parse(fs.readFileSync('.lighthouseci/assertion-results.json', 'utf8'));
                      const failedAssertions = assertionResults.filter(result => result.level === 'error' && !result.passed);
                      const warnAssertions = assertionResults.filter(result => result.level === 'warn' && !result.passed);
                      
                      if (failedAssertions.length > 0 || warnAssertions.length > 0) {
                        summaryContent += `## Performance Issues Found

            `;
                        
                        if (failedAssertions.length > 0) {
                          summaryContent += `### **${failedAssertions.length} Assertion Issues** (Must Fix if applicable for deployment aswell)

            `;
                          
                          // Group by URL
                          const errorsByUrl = {};
                          failedAssertions.forEach(assertion => {
                            const urlPath = new URL(assertion.url).pathname || '/';
                            const pageName = urlPath === '/' ? 'Homepage' : 
                                           urlPath === '/projects' ? 'Projects' :
                                           urlPath === '/blog' ? 'Blog' : 
                                           urlPath;
                            if (!errorsByUrl[pageName]) errorsByUrl[pageName] = [];
                            errorsByUrl[pageName].push(assertion);
                          });
                          
                          Object.entries(errorsByUrl).forEach(([page, assertions]) => {
                            summaryContent += `**${page}:**
            `;
                            assertions.forEach(assertion => {
                              summaryContent += `- **${assertion.auditTitle}** (Score: ${Math.round(assertion.actual * 100)}/100, Expected: ${Math.round(assertion.expected * 100)}/100)
            `;
                            });
                            summaryContent += `
            `;
                          });
                        }
                        
                        if (warnAssertions.length > 0) {
                          summaryContent += `### **${warnAssertions.length} Performance Warnings** (Should Fix)

            `;
                          
                          // Group by URL
                          const warnsByUrl = {};
                          warnAssertions.forEach(assertion => {
                            const urlPath = new URL(assertion.url).pathname || '/';
                            const pageName = urlPath === '/' ? 'Homepage' : 
                                           urlPath === '/projects' ? 'Projects' :
                                           urlPath === '/blog' ? 'Blog' : 
                                           urlPath;
                            if (!warnsByUrl[pageName]) warnsByUrl[pageName] = [];
                            warnsByUrl[pageName].push(assertion);
                          });
                          
                          Object.entries(warnsByUrl).forEach(([page, assertions]) => {
                            summaryContent += `**${page}:**
            `;
                            assertions.forEach(assertion => {
                              summaryContent += `- **${assertion.auditTitle}** (Score: ${Math.round(assertion.actual * 100)}/100, Expected: ${Math.round(assertion.expected * 100)}/100)
            `;
                            });
                            summaryContent += `
            `;
                          });
                        }
                        
                        summaryContent += `---

            `;
                      }
                    } catch (e) {
                      console.log('Error parsing assertion results:', e);
                    }
                  }
                  
                  // Parse each report file and group by URL
                  const reportsByUrl = {};
                  reportFiles.forEach((reportFile) => {
                    try {
                      const report = JSON.parse(fs.readFileSync(path.join(lhciDir, reportFile), 'utf8'));
                      const url = report.finalUrl || report.requestedUrl;
                      const urlPath = new URL(url).pathname || '/';
                      const pageName = urlPath === '/' ? 'Homepage' : 
                                     urlPath === '/projects' ? 'Projects' :
                                     urlPath === '/blog' ? 'Blog' : 
                                     urlPath;
                      
                      if (!reportsByUrl[pageName]) {
                        reportsByUrl[pageName] = [];
                      }
                      reportsByUrl[pageName].push({
                        url: url,
                        categories: report.categories || {}
                      });
                    } catch (e) {
                      console.log(`Error parsing report ${reportFile}:`, e);
                    }
                  });
                  
                  // Display results grouped by page
                  Object.entries(reportsByUrl).forEach(([pageName, reports]) => {
                    if (reports.length === 0) return;
                    
                    // Calculate average scores across runs
                    const avgScores = {
                      performance: 0,
                      accessibility: 0,
                      'best-practices': 0,
                      seo: 0,
                      pwa: 0
                    };
                    
                    reports.forEach(report => {
                      const categories = report.categories;
                      avgScores.performance += (categories.performance?.score || 0);
                      avgScores.accessibility += (categories.accessibility?.score || 0);
                      avgScores['best-practices'] += (categories['best-practices']?.score || 0);
                      avgScores.seo += (categories.seo?.score || 0);
                      avgScores.pwa += (categories.pwa?.score || 0);
                    });
                    
                    Object.keys(avgScores).forEach(key => {
                      avgScores[key] = Math.round((avgScores[key] / reports.length) * 100);
                    });
                    
                    summaryContent += `### ${pageName}
            **URL:** ${reports[0].url}
            **Runs:** ${reports.length}

            | Category | Score |
            |----------|--------|
            | Performance | ${avgScores.performance}/100 |
            | Accessibility | ${avgScores.accessibility}/100 |
            | Best Practices | ${avgScores['best-practices']}/100 |
            | SEO | ${avgScores.seo}/100 |
            | PWA | ${avgScores.pwa || 'N/A'}/100 |

            `;
                  });
                }
              }
              
              if (!summaryContent) {
                summaryContent = `## Lighthouse Performance Report
                
                Lighthouse CI completed for commit ${context.sha.substring(0, 8)}.
                Check the [workflow run](${context.payload.pull_request.html_url}/checks) for detailed results.
                `;
              }
            } catch (error) {
              summaryContent = `## Lighthouse Performance Report
              
              Lighthouse CI completed for commit ${context.sha.substring(0, 8)}.
              There was an issue parsing the results. Check the [workflow run](${context.payload.pull_request.html_url}/checks) for details.
              `;
            }
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summaryContent
            });
