name: Lighthouse Performance Testing

on:
  push:
    branches: [master]
  pull_request:
    branches: [master]

  workflow_dispatch:
    inputs:
      url:
        description: 'URL to test (defaults to production site)'
        required: false
        type: string

jobs:
  lighthouse:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: latest

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli@0.12.x

      # For manual runs with custom URL
      - name: Run Lighthouse CI (Custom URL)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.url
        run: |
          echo "Testing URL: ${{ github.event.inputs.url }}"
          
          # For manual runs, also test additional pages if it's a base URL
          BASE_URL="${{ github.event.inputs.url }}"
          if [[ "$BASE_URL" =~ ^https?://[^/]+/?$ ]]; then
            echo "Detected base URL, testing multiple pages..."
            PAGES=("" "/projects" "/blog")
            for page in "${PAGES[@]}"; do
              URL="${BASE_URL%/}$page"
              echo "Testing: $URL"
              lhci collect --url="$URL" --numberOfRuns=3 --startServerCommand="" --staticDistDir=""
            done
          else
            echo "Testing single URL..."
            lhci collect --url="$BASE_URL" --numberOfRuns=3 --startServerCommand="" --staticDistDir=""
          fi
          
          # Run assertions but capture result
          set +e  # Don't exit on error
          lhci assert
          ASSERT_EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          # Always upload results
          lhci upload --target=temporary-public-storage
          
          # Handle assertion failure
          if [ $ASSERT_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸  Performance budgets not met, but continuing workflow"
            echo "Check the detailed report for performance issues"
          else
            echo "âœ… All performance budgets passed!"
          fi
        env:
          LHCI_BUILD_CONTEXT__CURRENT_HASH: ${{ github.sha }}
          LHCI_BUILD_CONTEXT__COMMIT_TIME: ${{ github.event.head_commit.timestamp }}
          LHCI_BUILD_CONTEXT__CURRENT_BRANCH: ${{ github.ref_name }}
          LHCI_BUILD_CONTEXT__COMMIT_MESSAGE: ${{ github.event.head_commit.message }}
          LHCI_SERVER_URL: ${{ github.event.inputs.url }}
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}


      - name: Get the Vercel preview url
        id: vercel_preview_url
        uses: zentered/vercel-preview-url@v1.4.0
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        with:
          vercel_project_id: ${{ secrets.VERCEL_PROJECT_ID }}
      # For automatic runs, we'll wait for deployment and test the live site
      - name: Wait for deployment (Vercel/Netlify)
        if: github.event_name != 'workflow_dispatch'
        uses: UnlyEd/github-action-await-vercel@v2.0.0
        id: await-vercel
        with:
          deployment-url: ${{ format('https://{0}', steps.vercel_preview_url.outputs.preview_url) }}
          timeout: 300
          poll-interval: 5
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        continue-on-error: true


      - name: Run Lighthouse CI
        if: github.event_name != 'workflow_dispatch'
        run: |
          set -euo pipefail
          # Try to determine the deployment URL
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            # For PRs, construct preview URL (adjust domain as needed)
            DEPLOY_URL=${{ format('https://{0}', steps.vercel_preview_url.outputs.preview_url) }}
          else
            # For main branch, use production URL (adjust as needed)
            DEPLOY_URL="https://jaagop.eu"
          fi
          echo "Testing base URL: $DEPLOY_URL"
          
          # Define pages to test
          PAGES=("" "/projects" "/blog")
          
          # Test each page
          for page in "${PAGES[@]}"; do
            URL="$DEPLOY_URL$page"
            echo "Testing: $URL"
            lhci collect --url="$URL" --numberOfRuns=3 --startServerCommand="" --staticDistDir=""
          done
          
          # Run assertions but capture result
          set +e  # Don't exit on error
          lhci assert
          ASSERT_EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          # Always upload results
          lhci upload --target=temporary-public-storage
          
          # Handle assertion failure
          if [ $ASSERT_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸  Performance budgets not met, but continuing workflow"
            echo "Check the detailed report for performance issues"
          else
            echo "âœ… All performance budgets passed!"
          fi
        
        env:
          LHCI_BUILD_CONTEXT__CURRENT_HASH: ${{ github.sha }}
          LHCI_BUILD_CONTEXT__COMMIT_TIME: ${{ github.event.head_commit.timestamp }}
          LHCI_BUILD_CONTEXT__CURRENT_BRANCH: ${{ github.ref_name }}
          LHCI_BUILD_CONTEXT__COMMIT_MESSAGE: ${{ github.event.head_commit.message }}
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}

                # List of files in the .lighthouseci directory
      - name: List Lighthouse CI files
        if: always()
        run: |
          echo "Current working directory: $(pwd)"
          echo "Files in current directory:"
          ls -la
          echo "Files in .lighthouseci directory:"
          if [ -d ".lighthouseci" ]; then
            ls -la .lighthouseci/
          else
            echo ".lighthouseci directory does not exist."
          fi
          echo "Absolute path to .lighthouseci:"
          if [ -d ".lighthouseci" ]; then
            echo "$(pwd)/.lighthouseci"
          fi
          
      # Upload Lighthouse reports as artifacts
      - name: Upload Lighthouse reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-reports-${{ github.sha }}
          path: ".lighthouseci/*"
          retention-days: 30
          include-hidden-files: true


      # Comment on PR with results (if it's a PR)
      - name: Comment PR with Lighthouse results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            // Try to find the Lighthouse summary
            const lhciDir = '.lighthouseci';
            let summaryContent = '';
            try {
              if (fs.existsSync(lhciDir)) {
                const files = fs.readdirSync(lhciDir);
                const manifestFile = files.find(f => f.includes('manifest.json'));
                
                if (manifestFile) {
                  const manifest = JSON.parse(fs.readFileSync(path.join(lhciDir, manifestFile), 'utf8'));
                  const reports = manifest.filter(item => item.url && item.summary);
                  
                  if (reports.length > 0) {
                    summaryContent = `## ğŸš€ Lighthouse Performance Report

            **Tested ${reports.length} page(s) for commit ${context.sha.substring(0, 8)}**

            `;
                    
                    // Check if there are assertion failures
                    const hasAssertionFailures = fs.existsSync('.lighthouseci/assertion-results.json');
                    if (hasAssertionFailures) {
                      try {
                        const assertionResults = JSON.parse(fs.readFileSync('.lighthouseci/assertion-results.json', 'utf8'));
                        const failedAssertions = assertionResults.filter(result => result.level === 'error' && !result.passed);
                        
                        if (failedAssertions.length > 0) {
                          summaryContent += `âš ï¸ **${failedAssertions.length} performance budget(s) failed**

            `;
                        }
                      } catch (e) {
                        // Ignore parsing errors
                      }
                    }
                    
                    // Add results for each page
                    reports.forEach((report, index) => {
                      const summary = report.summary;
                      const urlPath = new URL(report.url).pathname || '/';
                      const pageName = urlPath === '/' ? 'Homepage' : 
                                     urlPath === '/projects' ? 'Projects' :
                                     urlPath === '/blog' ? 'Blog' : 
                                     urlPath;
                      
                      summaryContent += `### ${pageName}
            **URL:** ${report.url}

            | Category | Score |
            |----------|--------|
            | ğŸ”¥ Performance | ${Math.round(summary.performance * 100)}/100 |
            | â™¿ Accessibility | ${Math.round(summary.accessibility * 100)}/100 |
            | ğŸ’¡ Best Practices | ${Math.round(summary['best-practices'] * 100)}/100 |
            | ğŸ” SEO | ${Math.round(summary.seo * 100)}/100 |
            | âš¡ PWA | ${summary.pwa ? Math.round(summary.pwa * 100) : 'N/A'}/100 |

            `;
                    });
                  }
                }
              }
              
              if (!summaryContent) {
                summaryContent = `## ğŸš€ Lighthouse Performance Report
                
                Lighthouse CI completed for commit ${context.sha.substring(0, 8)}.
                Check the [workflow run](${context.payload.pull_request.html_url}/checks) for detailed results.
                `;
              }
            } catch (error) {
              summaryContent = `## ğŸš€ Lighthouse Performance Report
              
              Lighthouse CI completed for commit ${context.sha.substring(0, 8)}.
              There was an issue parsing the results. Check the [workflow run](${context.payload.pull_request.html_url}/checks) for details.
              `;
            }
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summaryContent
            });